<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head> 
<title>Classification: Michael Rose</title>
<meta name="keywords" content="" />
<meta name="description" content="" />
<meta name="author" content="" />
<meta http-equiv="content-type" content="text/html;charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<link rel="stylesheet" href="css/blueprint/screen.css" type="text/css" media="screen, projection" />
<link rel="stylesheet" href="css/blueprint/print.css" type="text/css" media="print" />
<link rel="stylesheet" href="css/main.css" type="text/css" media="screen" /> 
<!--[if IE]>
  <link rel="stylesheet" href="css/blueprint/ie.css" type="text/css" media="screen, projection">
<![endif]-->
</head>
<body>
<div class="container">
  <h1>Data Mining Portfolio: Michael Rose</h1>
  <h2>Classification</h2>
  <p class="introduction">Classification is the process of sorting data objects into predefined categories. Unlike clustering, classification does have set classes at the beginning of the method. The goal is to take a set of known data and use it to train a classifier, verify this classifier with data not used in the training, and then use this classifier to predict the class labels of previously unknown data objects. One of the best examples of classification is determining if an email message is spam. This will be dicussed in the Bayesian Classifier.</p>
  <h3>Decision Tree Classifier</h3>
  <p>
    The decision tree classifier works by taking data and breaking it into boolean questions, such as "Is the person age > 38? Is the person male?" The algorithm could be likened to a game of Twenty Questions. A decision tree consists of root nodes, internal nodes, and leaf nodes. Leaf nodes is where the algorithm ends, these contain the class labels. The root and internal nodes are given decisions to make (such as the questions above) and this is used to slowly narrow down the class of a given data object. This kind of classifier is generated by iteratively purifying the leaf conditions (to decrease entropy, or increase information). Another common statistic applied to classifiers is called the Gini Impurity. By applying these kinds of statistics, the iterative algorithm to generate these trees can be terminated when the differential of impurities falls below a threshhold. Unfortunately, this kind of classifier can be abused into overfitting or underfitting the data quite easily. Overfitting the model makes the model too specific and makes it impossible to sort new data, underfitting is just underwhelmingly bad at fitting the data specifically enough.
    <br />
    <img alt="gini impurity" src="images/classification/gini.png" /><br />
    <img alt="information" src="images/classification/information.png" /><br /><br /><br /><br /><br /><br /><br /><br />
  </p>
  <h3>Rule-Based Classifier</h3>
  <p>
    A rule-based classifier classifies data through a series of if-then rules. Rules are ranked by some priority (generally decided by humans or algorithms). An example of this kind of classifier would be checking if an aquatic creature is a mammal. Given that the creature has hair, it can be classified as a mammal. Many times these rules are extracted from other sources such as decision trees. As such, they can generally be represented as a series of OR statements, or the more canonical format of "disjunctive normal form", R = (r1 V r2 V r2 V ... V rn). Rule extraction can be done using sequential covering or learn-one- rule. Sequential covering is a greedy process which grows the rule database as exceptions are found which runs a risk of model overfitting. There are other strategies of sequential covering, such as specific-to-general rules which randomly chooses a specific rule and attempts to generalize it, general-to-specific does the process in reverse. It takes into account the accuracy of the rule and stops when the accuracy is good enough or the gains are too small. This model can easily become too specific, but like decision trees are fast and handle missing values.
  </p>
  <h3>Nearest Neighbor Classifier</h3>
  <p>
    Nearest neighbor classification is a lazy method. By this, it means it doesn't actually learn to classify data until the test data is used on the classifer (unlike most other methods). The NN classifier works by treating each object as a point in n-dimensional space, and then finding the point closest to it and classifying it as that point. Generally, this classifer is used as a k-nearest neighbor classifier. k nodes are sampled and a simple majority is found, thus if the point is near 3 others points, 2 being class A and 1 being class B, the point is classified as class A. The downside of this approach is that it's computationally intensive due to it's lazy training, unlike other models like Decision Tree, Rule-Based, and ANN which require up-front training but are relatively cheap to execute thereafter.
  </p>
  <h3>Bayesian Classifier</h3>
  <p>
    Naive Bayesian Classifiers work off the assumption that each attribute of an object (such as a fruit) are all probabalistically significant in determining the type of that object. Furthermore, it assumes that each attribute is independent (that the size of a fruit does not influence other parts). Given these independence guarantees it becomes simple with Thomas Bayes's seminal theory which allows conditional probabilities to be converted. P(A|B) = P(B|A)*P(A)/P(B). This being shown, we can use this to demonstratibly determine (with some probability) that our object is some data object. Naive Bayesian classifiers are used often in the detection of spam email using a number of generated feature about emails (such as the subject in ALL CAPS). An implementation of a Bayesian Classifier is <a href="http://en.wikipedia.org/wiki/SpamBayes">SpamBayes</a>.
  </p>
  <h3>Artifical Neural Network (ANN)</h3>
  <p>
    ANNs are applications of network flows using directed links. Perceptrons, a type of ANN, have input nodes which represent inputs and output nodes are the outputs and the links between them are weighted. These weights are initially randomly set, but are trained by feeding an algorithm a set of (input,output) pairs it is expected to know. For each link it then tweaks the weight until all input/output pairs or satisified, or if this is impossible it adds nodes. Generally this is achieved with an ANN known as a MLP (Multi-Layer Perceptron) which has input nodes, output nodes, and hidden nodes. Input is given to the input nodes which output with some weight to the hidden nodes (of which, there are generally far more of these than input and output nodes combined). Hidden nodes are added as required in order to support the training data. This training is usually accomplished using an algorithm known as back-propagation which the weights are updated using the weights of previous training runs and then applies an updated weight in the reverse direction. ANNs are fast, easy to use, and can easily adapt to data it's never seen before and reasonably guess at an answer. An example of a feed-forward multi-layer perceptron ANN using backpropagation can be found at <a href="https://github.com/Xorlev/csci568/tree/master/project07">https://github.com/Xorlev/csci568/tree/master/project07</a>.

    <img alt="MLP ANN" src="images/classification/ann.png" />
  </p>
</div>
</body>
</html>